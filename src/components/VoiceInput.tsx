import { useState, useEffect, useRef } from 'react';
import { Mic, Square } from 'lucide-react';

interface VoiceInputProps {
  onTranscript: (text: string) => void;
  onComplete?: () => void;
  onStart?: () => void;
  language?: string;
}

export function VoiceInput({ onTranscript, onComplete, onStart, language = 'fr-FR' }: VoiceInputProps) {
  const [isListening, setIsListening] = useState(false);
  const [transcript, setTranscript] = useState('');
  const [isSupported, setIsSupported] = useState(false);
  const recognitionRef = useRef<any>(null);
  const isListeningRef = useRef(false);
  const fullTranscriptRef = useRef('');
  const restartTimeoutRef = useRef<any>(null);
  const isInitializedRef = useRef(false);

  useEffect(() => {
    const SpeechRecognition = (window as any).SpeechRecognition || (window as any).webkitSpeechRecognition;
    
    if (!SpeechRecognition) {
      console.log('‚ùå Speech Recognition non support√©');
      setIsSupported(false);
      return;
    }

    console.log('‚úÖ Speech Recognition support√©, initialisation...');
    setIsSupported(true);
    
    // Initialiser la reconnaissance
    const recognition = new SpeechRecognition();
    recognition.continuous = true;
    recognition.interimResults = true;
    recognition.lang = language;
    recognition.maxAlternatives = 1;

    recognition.onstart = () => {
      console.log('‚úÖ Reconnaissance d√©marr√©e');
      setIsListening(true);
      isListeningRef.current = true;
      fullTranscriptRef.current = '';
      setTranscript('');
      if (onStart) {
        onStart();
      }
    };

    recognition.onresult = (event: any) => {
      let interimTranscript = '';

      // CRITIQUE: Parcourir depuis event.resultIndex jusqu'√† la fin
      // event.resultIndex indique o√π commencer (pour √©viter de traiter les m√™mes r√©sultats)
      for (let i = event.resultIndex; i < event.results.length; i++) {
        const transcript = event.results[i][0].transcript;
        if (event.results[i].isFinal) {
          // CRITIQUE: Ajouter au transcript final accumul√© avec += (NE PAS REMPLACER)
          const before = fullTranscriptRef.current;
          fullTranscriptRef.current += transcript + ' ';
          console.log('‚úÖ R√©sultat final #' + i + ':', transcript);
          console.log('‚úÖ Avant:', before);
          console.log('‚úÖ Apr√®s:', fullTranscriptRef.current);
        } else {
          // R√©sultat interm√©diaire - remplacer le pr√©c√©dent
          interimTranscript = transcript;
        }
      }

      // Combiner le transcript final accumul√© avec le dernier r√©sultat interm√©diaire
      const fullTranscript = fullTranscriptRef.current.trim() + (interimTranscript ? ' ' + interimTranscript : '');
      setTranscript(fullTranscript);
      
      console.log('üìù ===== TRANSCRIPT =====');
      console.log('üìù Final accumul√©:', fullTranscriptRef.current);
      console.log('üìù Longueur accumul√©e:', fullTranscriptRef.current.length);
      console.log('üìù Interm√©diaire:', interimTranscript);
      console.log('üìù Transcript complet:', fullTranscript);
      console.log('üìù Longueur totale:', fullTranscript.length);
      console.log('üìù ======================');
      
      // Appeler onTranscript avec le transcript complet
      if (fullTranscript.trim().length > 0) {
        onTranscript(fullTranscript.trim());
      }
    };

    recognition.onerror = (event: any) => {
      console.error('‚ùå Erreur reconnaissance:', event.error);
      
      if (event.error === 'no-speech') {
        console.log('‚ö†Ô∏è Pas de parole, continuation...');
        return;
      }
      
      if (event.error === 'aborted') {
        console.log('‚ö†Ô∏è Reconnaissance interrompue');
        if (isListeningRef.current) {
          setTimeout(() => {
            if (isListeningRef.current && recognitionRef.current) {
              try {
                console.log('üîÑ Red√©marrage apr√®s interruption...');
                recognitionRef.current.start();
              } catch (error) {
                console.error('‚ùå Erreur red√©marrage:', error);
              }
            }
          }, 500);
        }
        return;
      }
      
      if (event.error === 'network') {
        console.error('‚ùå Erreur r√©seau');
        setIsListening(false);
        isListeningRef.current = false;
        return;
      }
    };

    recognition.onend = () => {
      console.log('‚ö†Ô∏è Reconnaissance termin√©e');
      
      if (restartTimeoutRef.current) {
        clearTimeout(restartTimeoutRef.current);
      }
      
      if (isListeningRef.current && recognitionRef.current) {
        restartTimeoutRef.current = setTimeout(() => {
          if (isListeningRef.current && recognitionRef.current) {
            try {
              console.log('üîÑ Red√©marrage automatique...');
              recognitionRef.current.start();
            } catch (error: any) {
              if (error.message?.includes('already started')) {
                console.log('‚úÖ D√©j√† d√©marr√©');
              } else {
                console.error('‚ùå Erreur red√©marrage:', error);
                setIsListening(false);
                isListeningRef.current = false;
              }
            }
          }
        }, 100);
      } else {
        console.log('‚èπÔ∏è Arr√™t manuel confirm√©');
        setIsListening(false);
      }
    };

    recognitionRef.current = recognition;
    isInitializedRef.current = true;
    console.log('‚úÖ Reconnaissance initialis√©e');

    return () => {
      console.log('üßπ Nettoyage de la reconnaissance');
      if (restartTimeoutRef.current) {
        clearTimeout(restartTimeoutRef.current);
      }
      if (recognitionRef.current) {
        try {
          recognitionRef.current.stop();
          recognitionRef.current.abort();
        } catch (error) {
          // Ignorer les erreurs de nettoyage
        }
      }
    };
  }, [language, onTranscript, onStart]);

  const startListening = async (e?: React.MouseEvent) => {
    if (e) {
      e.preventDefault();
      e.stopPropagation();
    }
    
    console.log('üöÄ D√©marrage demand√©, isListening:', isListening);
    
    if (!recognitionRef.current) {
      console.error('‚ùå Reconnaissance non initialis√©e');
      alert('La reconnaissance vocale n\'est pas initialis√©e. Rechargez la page.');
      return;
    }
    
    if (isListening) {
      console.log('‚ö†Ô∏è D√©j√† en √©coute');
      return;
    }
    
    try {
      // Demander l'autorisation du micro
      try {
        console.log('üîê Demande d\'autorisation micro...');
        await navigator.mediaDevices.getUserMedia({ audio: true });
        console.log('‚úÖ Autorisation micro accord√©e');
      } catch (mediaError: any) {
        console.error('‚ùå Erreur autorisation micro:', mediaError);
        if (mediaError.name === 'NotAllowedError' || mediaError.name === 'PermissionDeniedError') {
          alert('L\'autorisation d\'utiliser le micro est requise pour la saisie vocale.');
          return;
        }
        throw mediaError;
      }
      
      // D√©marrer la reconnaissance
      console.log('üöÄ D√©marrage de la reconnaissance...');
      recognitionRef.current.start();
      console.log('‚úÖ Commande start() envoy√©e');
    } catch (error: any) {
      console.error('‚ùå Erreur d√©marrage:', error);
      if (error.message?.includes('already started')) {
        console.log('‚úÖ D√©j√† d√©marr√©');
        setIsListening(true);
        isListeningRef.current = true;
      } else {
        alert('Erreur lors du d√©marrage de la reconnaissance vocale: ' + (error.message || 'Erreur inconnue'));
      }
    }
  };

  const stopListening = (e?: React.MouseEvent) => {
    if (e) {
      e.preventDefault();
      e.stopPropagation();
    }
    
    console.log('‚èπÔ∏è Arr√™t de l\'√©coute demand√©');
    
    isListeningRef.current = false;
    setIsListening(false);
    
    if (restartTimeoutRef.current) {
      clearTimeout(restartTimeoutRef.current);
      restartTimeoutRef.current = null;
    }
    
    if (fullTranscriptRef.current.trim().length > 0) {
      console.log('üì§ Envoi du transcript final:', fullTranscriptRef.current);
      onTranscript(fullTranscriptRef.current);
    }
    
    if (recognitionRef.current) {
      try {
        recognitionRef.current.stop();
        recognitionRef.current.abort();
        console.log('‚úÖ Reconnaissance arr√™t√©e');
      } catch (error) {
        console.log('‚ö†Ô∏è Erreur lors de l\'arr√™t (non critique):', error);
      }
    }
  };

  if (!isSupported) {
    return (
      <div className="p-4 bg-yellow-500/20 border border-yellow-500/30 rounded-lg">
        <p className="text-yellow-400 text-sm">
          ‚ö†Ô∏è La reconnaissance vocale n'est pas support√©e par votre navigateur.
          <br />
          Utilisez Chrome, Edge ou Safari pour cette fonctionnalit√©.
        </p>
      </div>
    );
  }

  return (
    <div 
      className="space-y-3"
      onClick={(e) => {
        e.stopPropagation();
        e.preventDefault();
        e.nativeEvent.stopImmediatePropagation();
      }}
      onMouseDown={(e) => {
        e.stopPropagation();
        e.nativeEvent.stopImmediatePropagation();
      }}
      onMouseUp={(e) => {
        e.stopPropagation();
        e.nativeEvent.stopImmediatePropagation();
      }}
    >
      <div className="flex items-center gap-3">
        <button
          key={isListening ? 'stop' : 'start'}
          type="button"
          onClick={async (e) => {
            console.log('üñ±Ô∏è Bouton cliqu√©, isListening:', isListening);
            
            e.preventDefault();
            e.stopPropagation();
            if (e.nativeEvent) {
              e.nativeEvent.stopImmediatePropagation();
            }
            if ((e as any).cancelable !== false) {
              (e as any).cancelBubble = true;
            }
            
            if (!isListening && onStart) {
              console.log('üìû Appel de onStart()');
              onStart();
            }
            
            if (isListening) {
              stopListening(e);
            } else {
              await startListening(e);
            }
          }}
          onMouseDown={(e) => {
            e.preventDefault();
            e.stopPropagation();
            if (e.nativeEvent) {
              e.nativeEvent.stopImmediatePropagation();
            }
            if ((e as any).cancelable !== false) {
              (e as any).cancelBubble = true;
            }
            if (!isListening && onStart) {
              onStart();
            }
          }}
          onMouseUp={(e) => {
            e.preventDefault();
            e.stopPropagation();
            if (e.nativeEvent) {
              e.nativeEvent.stopImmediatePropagation();
            }
            if ((e as any).cancelable !== false) {
              (e as any).cancelBubble = true;
            }
          }}
          className={`flex items-center gap-2 px-4 py-2 rounded-lg font-medium transition-all ${
            isListening
              ? 'bg-red-500 hover:bg-red-600 text-white'
              : 'bg-blue-500 hover:bg-blue-600 text-white'
          }`}
        >
          {isListening ? (
            <span className="flex items-center gap-2">
              <Square className="w-4 h-4" />
              <span>Arr√™ter</span>
            </span>
          ) : (
            <span className="flex items-center gap-2">
              <Mic className="w-4 h-4" />
              <span>Parler</span>
            </span>
          )}
        </button>
        
        {isListening && (
          <div className="flex items-center gap-2 text-red-400">
            <div className="w-2 h-2 bg-red-400 rounded-full animate-pulse"></div>
            <span className="text-sm">En √©coute...</span>
          </div>
        )}
      </div>

      {transcript && (
        <div className="p-3 bg-white/5 rounded-lg border border-white/10">
          <p className="text-sm text-gray-300">
            <span className="font-semibold">Transcription :</span>
            <br />
            <span className="text-white">{transcript}</span>
          </p>
        </div>
      )}
    </div>
  );
}
